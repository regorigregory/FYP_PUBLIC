{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  8\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# Built-in modules ################################################\n",
    "###################################################################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import numpy as np\n",
    "import imp\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "\n",
    "###################################################################\n",
    "# Custom modules ##################################################\n",
    "###################################################################\n",
    "\n",
    "from components.matchers.NumbaPatchMatcher import Wrapper as m\n",
    "from components.utils.SimpleProgressBar import SimpleProgressBar\n",
    "from components.utils import utils as u\n",
    "from components.utils import plot_utils as plu\n",
    "from components.utils.CSVWriter2 import Wrapper as csv\n",
    "from components.utils.Metrix import Wrapper as me\n",
    "\n",
    "###################################################################\n",
    "# Dataset specific modules#########################################\n",
    "###################################################################\n",
    "\n",
    "from components.utils import middlebury_utils as mbu\n",
    "from components.utils import SimpleConvolution as SC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = window.location.href\n",
       "IPython.notebook.kernel.execute(\"notebook_url='\"+ url+\"'\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var url = window.location.href\n",
    "IPython.notebook.kernel.execute(\"notebook_url='\"+ url+\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALG_005_EXP_005-Naive_preprocessing'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_name = notebook_url.split(\"/\")[-1].split(\".\")[0]\n",
    "nb_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_TITLE = nb_name.split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROOT_PATH = os.path.join(\"..\", \"..\")\n",
    "\n",
    "INIT_METHOD = \"maclean\"\n",
    "DATASET = \"middlebury\"\n",
    "\n",
    "DATASET_FOLDER = os.path.join(ROOT_PATH, \"datasets\", DATASET)\n",
    "LOG_FOLDER = os.path.join(ROOT_PATH, \"experiments\", \"logs\")\n",
    "CSV_FILEPATH = os.path.join(LOG_FOLDER, EXPERIMENT_TITLE+\".csv\")\n",
    "IMG_RES = \"450X375\"\n",
    "PREPROCESSING_METHOD = \"None\"\n",
    "KERNEL_SIZE = 1\n",
    "KERNEL_SPEC = \"None\"\n",
    "\n",
    "SCENES = [\"teddy\", \"cones\"]\n",
    "SIZE=\"\"\n",
    "YEAR= 2003\n",
    "EXP_PARAMS = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_imgs_and_paths = list(mbu.get_images(DATASET_FOLDER, YEAR, scene) for scene in SCENES)\n",
    "\n",
    "for im, path in loaded_imgs_and_paths:\n",
    "    filenames = list(os.path.split(p)[-1] for p in path)\n",
    "    plu.plot_images(im, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesssed_dataset = {k:None for k in SCENES}\n",
    "\n",
    "for i, scene in enumerate(SCENES):\n",
    "    convolver = SC.getOne()\n",
    "    \n",
    "    im2 = loaded_imgs_and_paths[i][0][0]\n",
    "    im6 = loaded_imgs_and_paths[i][0][1]\n",
    "\n",
    "    im2_blurred = convolver.convolve(im2)\n",
    "    im6_blurred = convolver.convolve(im6)\n",
    "\n",
    "    u.getHorizontalFeatureFilter(convolver)\n",
    "\n",
    "    horizontal_feature = convolver.filter\n",
    "\n",
    "    im2_h = convolver.convolve(im2)\n",
    "    im6_h = convolver.convolve(im6)\n",
    "\n",
    "    u.getVerticalFeatureFilter(convolver)\n",
    "\n",
    "    vertical_feature = convolver.filter\n",
    "\n",
    "    im2_v = convolver.convolve(im2)\n",
    "    im6_v = convolver.convolve(im6)\n",
    "\n",
    "    u.getFilterByTypo(convolver)\n",
    "\n",
    "    typo_filter = convolver.filter\n",
    "\n",
    "    im2_t = convolver.convolve(im2)\n",
    "    im6_t = convolver.convolve(im6)\n",
    "\n",
    "\n",
    "    im2_features_added = im2+im2+im2_h+im2_t\n",
    "    im6_features_added = im6+im6+im6_h+im6_t\n",
    "    \n",
    "    template_dict = dict(original=[im2, im6], \n",
    "                         blurred=[im2_blurred, im6_blurred], \n",
    "                         horizontal=[im2_h,im6_h], \n",
    "                         vertical=[im2_v, im6_v], \n",
    "                         typo=[im2_t, im6_t],\n",
    "                         all_features=[im2_features_added,im6_features_added]\n",
    "                        )\n",
    "    preprocesssed_dataset[scene] = template_dict\n",
    "    \n",
    "plt.subplots_adjust(wspace=3.0)\n",
    "ax = plt.subplot(131)\n",
    "ax.set_title(\"Horizontal feature filter\")\n",
    "plt.imshow(horizontal_feature, \"gray\")\n",
    "ax = plt.subplot(132)\n",
    "ax.set_title(\"Vertical feature filter\")\n",
    "plt.imshow(vertical_feature, \"gray\")\n",
    "ax = plt.subplot(133)\n",
    "\n",
    "ax.set_title(\"Feature resulted by a typo\")\n",
    "plt.imshow(typo_filter, \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effect of applying the named filters to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.subplots(3,3, figsize=[20, 6])\n",
    "plt.subplots_adjust(hspace=0.2, wspace=0.5)\n",
    "plot_counter = 1\n",
    "\n",
    "for k,v in preprocesssed_dataset.items():\n",
    "    for k_i,v_i in v.items():\n",
    "        left = v_i[0]\n",
    "        ax = plt.subplot(2,6, plot_counter)\n",
    "        plot_counter+=1\n",
    "        ax.set_title(k+\": \"+k_i)\n",
    "        plt.imshow(left, cmap=\"gray\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge detection filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Initialising hyperparameters and matcher#####################\n",
    "###################################################################\n",
    "\n",
    "csv_logger = csv(CSV_FILEPATH, default_header=False)\n",
    "csv_logger.set_header_function(csv_logger.get_header_v3)\n",
    "csv_logger.write_csv_header()\n",
    "csv_logger.set_line_function(csv.format_stereo_matching_results_v2)\n",
    "\n",
    "MATCH = 60\n",
    "GAP = -20\n",
    "EGAP = -1\n",
    "\n",
    "\n",
    "EXP_PARAMS = {\"experiment_id\":EXPERIMENT_TITLE, \"match\": MATCH,\"gap\":GAP,\"egap\":EGAP,\\\n",
    "                  \"algo\":str(m.__module__), \"init_method\":\"default\",\"dataset\":DATASET,\\\n",
    "                  \"preprocessing_method\":\"None\",\"kernel_size\":1,\"kernel_spec\":\"None\"}\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# Matching ########################################################\n",
    "###################################################################\n",
    "\n",
    "EXP_PARAMS[\"init_method\"] = \"maclean_et_al\"\n",
    "\n",
    "\n",
    "\n",
    "patches = [np.ones((1,1)), np.ones((3,3)), np.ones((5,5)), np.ones((7,7)),\n",
    "           np.ones((3,1)), np.ones((1,3)), np.ones((5,1)), np.ones((1,5)), np.ones((7,1)), np.ones((1,7)),\n",
    "           np.ones((5,3)), np.ones((3,5)), np.ones((7,3)), np.ones((3,7)),\n",
    "           np.ones((7,5)), np.ones((5,7)),\n",
    "           np.ones((9,3)), np.ones((11,3)),\n",
    "           np.ones((13,3)), np.ones((15,3))\n",
    "          ]\n",
    "\n",
    "\n",
    "progress_bar = SimpleProgressBar.get_instance()\n",
    "\n",
    "progress_counter = 1\n",
    "\n",
    "param_start = 10\n",
    "param_end= 120\n",
    "\n",
    "step=10\n",
    "main_loop_multiplier = math.floor((param_end-param_start)/step)\n",
    "\n",
    "steps_to_be_taken = len(SCENES)*6*len(patches)*main_loop_multiplier\n",
    "\n",
    "\n",
    "\n",
    "SAVE_PATH  = os.path.join(ROOT_PATH, \"experiments\", \"disparities\", EXPERIMENT_TITLE)\n",
    "\n",
    "saved_image_names = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Running the experiment ##########################################\n",
    "###################################################################\n",
    "\n",
    "\n",
    "for match_value in (param_start,param_end, step):\n",
    "    \n",
    "    EXP_PARAMS[\"match\"] = MATCH = match_value\n",
    "    matcher = m(MATCH, GAP, EGAP)\n",
    "    matcher.configure_instance()\n",
    "    \n",
    "    i=-1\n",
    "    \n",
    "    for k,v in preprocesssed_dataset.items():\n",
    "        EXP_PARAMS[\"scene\"] = k\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        for k_i, v_i in v.items():\n",
    "            EXP_PARAMS[\"preprocessing_method\"] = k_i\n",
    "\n",
    "            for p in patches:\n",
    "\n",
    "                matcher.set_filter(p)\n",
    "\n",
    "                EXP_PARAMS[\"img_res\"] = \"{0}x{1}\".format(v_i[0].shape[1], v_i[0].shape[0])  \n",
    "                EXP_PARAMS[\"kernel_size\"] = \"{0}x{1}\".format(p.shape[0], p.shape[1])\n",
    "\n",
    "                occ = loaded_imgs_and_paths[i][0][3]\n",
    "                gt = loaded_imgs_and_paths[i][0][2]\n",
    "\n",
    "                left = v_i[0]\n",
    "                right = v_i[1]\n",
    "                \n",
    "                matcher.set_images(left, right)\n",
    "\n",
    "                tic = time.time()\n",
    "\n",
    "                x,raw_disp_map = matcher.test_pipeline()\n",
    "                toc = time.time()\n",
    "\n",
    "                EXP_PARAMS[\"runtime\"] = toc-tic\n",
    "\n",
    "                disp = raw_disp_map\n",
    "                disp = 4*disp\n",
    "\n",
    "                results.append(disp)\n",
    "                temp_path = u.save_disparity(SAVE_PATH, disp)\n",
    "                saved_image_names.append(os.path.split(temp_path)[-1])\n",
    "\n",
    "                EXP_PARAMS[\"image_filename\"] = temp_path\n",
    "\n",
    "                EXP_PARAMS[\"are_occlusions_errors\"] = ARE_OCCLUSIONS_ERRORS = False\n",
    "\n",
    "                EXP_PARAMS[\"bad1\"], EXP_PARAMS[\"bad2\"], EXP_PARAMS[\"bad4\"], EXP_PARAMS[\"BAD8\"], EXP_PARAMS[\"abs_error\"], EXP_PARAMS[\"mse\"], EXP_PARAMS[\"avg\"], EXP_PARAMS[\"eucledian\"] =\\\n",
    "                BAD1, BAD2, BAD4, BAD8, ABS_ERR, MSE, AVG, EUCLEDIAN = me.evaluate_over_all(disp, gt, occ, occlusions_counted_in_errors = ARE_OCCLUSIONS_ERRORS)\n",
    "\n",
    "                csv_logger.append_new_sm_results(EXP_PARAMS, selected_keys=csv.get_header_v3())\n",
    "\n",
    "                EXP_PARAMS[\"are_occlusions_errors\"]  = True\n",
    "                EXP_PARAMS[\"bad1\"], EXP_PARAMS[\"bad2\"], EXP_PARAMS[\"bad4\"], EXP_PARAMS[\"BAD8\"], EXP_PARAMS[\"abs_error\"], EXP_PARAMS[\"mse\"], EXP_PARAMS[\"avg\"], EXP_PARAMS[\"eucledian\"] =\\\n",
    "                BAD1, BAD2, BAD4, BAD8, ABS_ERR, MSE, AVG, EUCLEDIAN = me.evaluate_over_all(disp, gt, gt, occlusions_counted_in_errors = ARE_OCCLUSIONS_ERRORS)\n",
    "\n",
    "                csv_logger.append_new_sm_results(EXP_PARAMS, selected_keys=csv.get_header_v3())\n",
    "\n",
    "                progress_bar.progress_bar(progress_counter, steps_to_be_taken, header= \"Experiment on patch sizes in progress: \", progress_bar_steps=40)\n",
    "                progress_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "experiment_log = pd.read_csv(CSV_FILEPATH)\n",
    "experiment_log.sort_values(by=\"mse\", inplace=True) \n",
    "experiment_log.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plu.plot_images(results[0:50], saved_image_names[0:50], ncols= 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
