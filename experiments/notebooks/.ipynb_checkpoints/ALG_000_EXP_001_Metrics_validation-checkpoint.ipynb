{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"-Validation-of-the-implementation-of-the-Middlebury-2003-Framework's-metrics\">Validation of the implementation of the Middlebury 2003 Framework's metric</h1>\n",
    "<h2 id=\"Introduction\">Introduction</h2>\n",
    "<p>Before any experiment is conducted in the world of science, the tools and means used to measure the outcomes have to be calibrated and validated, additionally, metrics have to be unified. One noteworthy mishap&nbsp;as a result of overlooking this aspect happened at Nasa (Ajay Harish, 2020) . This incident was due to different metric systems used which eventually resulted in a loss of a spacecraft.</p>\n",
    "<p>The importance of this procedure is not less important when it comes to Computer Science and within that, Stereo Vision. As a first milestone the Middlebury 2003 dataset and its associated metrics were selected to be implemented.&nbsp;</p>\n",
    "<h2 id=\"Abstract\" style=\"font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #000000;\">Abstract</h2>\n",
    "<p>As a first step data was obtained from the Middlebury Evaluation (version 2) site entailing both disparities and scores of top-scoring algorithms. Then, based on the documentation available from this site, local version of the evaluation framework was implemented in python. The correctness of this implementation was cross checked by comparing the locally benchmarked results of the aforementioned disparity maps to their respective online results. It could be established that the local implementation was correct despite of the fact that discrepancies still persisted. It was assumed to be the result of the online chart&rsquo;s inconsistent fraction precision and rounding practices.</p>\n",
    "<h2 id=\"Relevant-theory\">Relevant theory</h2>\n",
    "<p>The Middlebury Stereo datasets (Scharstein <em>et al.</em>, 2014) became one of the cornerstone datasets of Stereo Vision starting in 2003. There have been 5 <a href=\"https://vision.middlebury.edu/stereo/data/\" target=\"_blank\" rel=\"noopener\">incremental versions</a>. The first dataset and its associated metrics decided to be used&nbsp;in this project was the&nbsp;Middlebury 2003 dataset (Scharstein and Szeliski, 2003). The description of the evaluation of this dataset and their associated metrics are published at the site \"<a href=\"https://vision.middlebury.edu/stereo/eval/\" target=\"_blank\" rel=\"noopener\">Middlebury Stereo Evaluation - Version 2</a>\" (Scharstein and Blasiak, 2003).&nbsp;This evaluation framework is now defunct, though, the submitted algorithms and their results are still available. The stereo \"Video-mics\" algorithms (Dieny <em>et al.</em>, 2011; Madeo <em>et al.</em>, 2016) were benchmarked using the second version of the named framework. Therefore, in order to be able to evaluate newly implemented algorithms part of this project comparatively and objectively, it was thought to be necessary.</p>\n",
    "<p>This framework used two images from a previous dataset (&ldquo;Tsukaba&rdquo; and &ldquo;Teddy&rdquo; from 2001). However, these images were not used. This dataset was thought to be small and simple enough to be used as an initial tool for debugging incidental errors in the implementation.</p>\n",
    "<h3 id=\"About-disparity-maps-and-used-metrics\">About disparity maps and used metrics</h3>\n",
    "<p>Occlusions are represented with 0 pixel intensities. Disparities at this resolution are encoded into &ldquo;png&rdquo; intensity levels at quarter pixel accuracy entailing that 4 intensity level difference in the ground truth image is equivalent to one disparity level change. The metrics used to evaluate the algorithms&rsquo; performance was the so called&nbsp;\"Bad N\".&nbsp;This measure calculates the threshold average absolute difference between the intensity values of the ground truth and newly submitted disparity maps. The&nbsp;\"N\"&nbsp;stands for a threshold value which is used to ignore absolute differences below that value. Version 2 results are published with threshold values ranging from 0.5 to 2.</p>\n",
    "<p>Additionally, occlusion masks were provided entailing another dimension for evaluation. If the evaluation is \"nonoccluded\", only pixels outside of the mask (occlusions) and unknown regions (disparity is unknown) are evaluated. Otherwise, occluded regions are included as well, though unknown pixels are still omitted.</p>\n",
    "<h2 id=\"Method\">Methodology</h2>\n",
    "<p>In order to establish equivalence between the local implementation of Middlebury 2003 metrics and its online counterpart (on top of unit testing mentioned above) the following steps were taken:</p>\n",
    "<ol>\n",
    "<li>Unit tests were performed to test simple scenarios of the implemented metrics.</li>\n",
    "<li>5 top benchmark results for each available threshold value (0.5, 0.75, 1, 1.5, 3) were collected from the Middlebury Evaluation Site's chart into a table. These results were achieved by 4 different algorithms, namely, <strong>IGSM </strong>(Zhan <em>et al.</em>, 2016)<strong>, MULTIRBF </strong>(Zhou and Boulanger, 2012)<strong> , SEGAGGR </strong>(Muninder, Soumik and Krishna, 2014)<strong> and GC+LocalExp </strong>(Taniai <em>et al.</em>, 2018)<strong> .&nbsp;</strong></li>\n",
    "<li>The disparity maps pertaining to these results were also obtained.</li>\n",
    "<li>Local evaluation of the disparity maps were done on two scenes (&ldquo;Teddy&rdquo; and &ldquo;Cones&rdquo;) both in a &ldquo;nonoccluded&rdquo; and &ldquo;all&rdquo; manner with threshold values 2, 4, 8 equating to results of 0.5, 1 and 4 in the online chart. Additional metrics, such as mean squared error and Eucledian distance are also observable in the notebook below due to programming convenience.&nbsp;</li>\n",
    "<li>As a final step, local evaluation results were compared to the online results according to two calculation: nonoccluded and all (entailing unknown regions).</li>\n",
    "</ol>\n",
    "<p>Please note that local benchmark results in this notebook do not consider the quarter pixel accuracy mentioned above, therefore the displayed &ldquo;bad4&rdquo; values are in fact equivalent to &ldquo;bad1&rdquo;.</p>\n",
    "<h2 id=\"Results-and-discussion\">Results and discussion</h2>\n",
    "<p>During the cross-checking the following obstacles were encountered:</p>\n",
    "<p>The first run resulted in discrepancies in for at least 50% of the comparisons. After manual analysis it was assumed that it was due to the online data&rsquo;s variance in precision between 1 and 2 fraction digits. This was overcome by rounding both the local and the online data to 1 fraction digit. However, upon a repeated comparison, the results still did not match in two instances. This was one decimal fraction difference in the Bad 2.0 metric for two algorithms&rsquo; nonoccluded scores. The reason could not be found empirically, though, it was assumed to be the result of the online data&rsquo;s inconsistent precision and rounding practices.</p>\n",
    "<h2 id=\"Conclusion\">Conclusion</h2>\n",
    "<p>The main objective of this experiment was to establish the correctness of the local implementation of the Middlebury Evaluation Framework (version 2). This implementation was done in python.uring the first run of the notebook a large number of discrepancies were encountered. After further manual analysis, it was inferred to be the result of inconsistencies in the precision of the published results. Applying a simple rounding to one fraction digit, the errors were totally eliminated, but in two cases. This was thought to be a consequence of additional inconsistencies in rounding practices in the published chart. It could be established that the local metrics were properly implemented.</p>\n",
    "<h2 id=\"References\">References</h2>\n",
    "<ol>\n",
    "<li>Ajay Harish (2020) <em>When NASA Lost a Spacecraft Due to a Metric Math Mistake</em>, <em>Simscale blog</em>. Available at: https://www.simscale.com/blog/2017/12/nasa-mars-climate-orbiter-metric/ (Accessed: 16 June 2020).</li>\n",
    "<li>Dieny, R. <em>et al.</em> (2011) &lsquo;Bioinformatics inspired algorithm for stereo correspondence&rsquo;, <em>VISAPP 2011 - Proceedings of the International Conference on Computer Vision Theory and Application</em>, pp. 465&ndash;473. doi: 10.5220/0003312304650473.</li>\n",
    "<li>Madeo, S. <em>et al.</em> (2016) &lsquo;An optimized stereo vision implementation for embedded systems: application to RGB and infra-red images&rsquo;, <em>Journal of Real-Time Image Processing</em>, 12(4), pp. 725&ndash;746. doi: 10.1007/s11554-014-0461-7.</li>\n",
    "<li>Muninder, V., Soumik, U. and Krishna, G. (2014) &lsquo;Robust segment-based stereo using cost aggregation&rsquo;, in <em>BMVC 2014 - Proceedings of the British Machine Vision Conference 2014</em>. British Machine Vision Association, BMVA. doi: 10.5244/c.28.40.</li>\n",
    "<li>Scharstein, D. <em>et al.</em> (2014) <em>Middlebury Stereo Datasets</em>. Available at: http://vision.middlebury.edu/stereo/data/ (Accessed: 17 October 2019).</li>\n",
    "<li>Scharstein, D. and Blasiak, A. (2003) <em>Middlebury Stereo Evaluation - Version 2</em>, <em>Middlebury Stereo Site</em>. Available at: http://vision.middlebury.edu/stereo/eval/ (Accessed: 17 June 2020).</li>\n",
    "<li>Scharstein, D. and Szeliski, R. (2003) <em>2003 Stereo Datasets</em>. Available at: http://vision.middlebury.edu/stereo/data/scenes2003/ (Accessed: 17 October 2019).</li>\n",
    "<li>Taniai, T. <em>et al.</em> (2018) &lsquo;Continuous 3D Label Stereo Matching Using Local Expansion Moves&rsquo;, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>. IEEE Computer Society, 40(11), pp. 2725&ndash;2739. doi: 10.1109/TPAMI.2017.2766072.</li>\n",
    "<li>Zhan, Y. <em>et al.</em> (2016) &lsquo;Accurate Image-Guided Stereo Matching with Efficient Matching Cost and Disparity Refinement&rsquo;, <em>IEEE Transactions on Circuits and Systems for Video Technology</em>. Institute of Electrical and Electronics Engineers Inc., 26(9), pp. 1632&ndash;1645. doi: 10.1109/TCSVT.2015.2473375.</li>\n",
    "<li>Zhou, X. and Boulanger, P. (2012) &lsquo;New eye contact correction using radial basis function for wide baseline videoconference system&rsquo;, in <em>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</em>. Springer, Berlin, Heidelberg, pp. 68&ndash;79. doi: 10.1007/978-3-642-34778-8_7.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Importing built-in modules ######################################\n",
    "###################################################################\n",
    "\n",
    "import os\n",
    "\n",
    "ROOT=os.path.join(\"..\", \"..\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import glob\n",
    "import re\n",
    "\n",
    "###################################################################\n",
    "# Importing custom modules ######################################\n",
    "###################################################################\n",
    "\n",
    "\n",
    "from components.utils import utils as u\n",
    "from components.utils.Metrix import Wrapper as me\n",
    "from components.utils import plot_utils as plu\n",
    "from components.utils.CSVWriter2 import Wrapper as csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 0.069s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run ../../tests/metrix_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16 entries, 0 to 15\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   ALG_NAME     16 non-null     object \n",
      " 1   IMG          16 non-null     object \n",
      " 2   NONOCCLUDED  16 non-null     int64  \n",
      " 3   BAD_0.5      16 non-null     float64\n",
      " 4   BAD_1.0      16 non-null     float64\n",
      " 5   BAD_2.0      16 non-null     float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 896.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# Initialising path variables #####################################\n",
    "###################################################################\n",
    "\n",
    "\n",
    "ROOT_PATH = os.path.join(\"..\", \"..\")\n",
    "EXPERIMENT_TITLE = \"Metrics_001_validation\"\n",
    "\n",
    "LOG_FOLDER = os.path.join(ROOT_PATH, \"experiments\", \"logs\")\n",
    "CSV_FILEPATH = os.path.join(LOG_FOLDER, EXPERIMENT_TITLE+\".csv\")\n",
    "\n",
    "\n",
    "IMAGE_DIRECTORY = os.path.join(ROOT_PATH, \"tests\", \"metrix_test\") \n",
    "SELECTED_METRIC = \"bad*\"\n",
    "IMG_FORMAT=\"*.png\"\n",
    "\n",
    "\n",
    "images_to_test = [\"venus\", \"tsukaba\", \"teddy\", \"cones\"]\n",
    "image_dict = dict({\"venus\": [], \"tsukaba\":[], \"teddy\":[], \"cones\":[]})\n",
    "\n",
    "header_func = lambda  : \"ALG_NAME,THRESHOLD,IMG,NONOCC,ALL\".split(\",\")\n",
    "\n",
    "\n",
    "csv_logger = csv(CSV_FILEPATH, default_header=False)\n",
    "csv_logger.set_header_function(header_func)\n",
    "csv_logger.write_csv_header()\n",
    "csv_logger.set_line_function(csv.format_stereo_matching_results_v2)\n",
    "\n",
    "###################################################################\n",
    "# Scanning directory for disparities###############################\n",
    "###################################################################\n",
    "\n",
    "folders = glob.glob(os.path.join(IMAGE_DIRECTORY, SELECTED_METRIC, IMG_FORMAT))\n",
    "\n",
    "for key, value in image_dict.items():\n",
    "    r = re.compile(\".*\"+key+\".*\")\n",
    "    matches = list(filter(r.match, folders))\n",
    "    image_dict[key] = matches\n",
    "\n",
    "\n",
    "gt_files = sorted(glob.glob(os.path.join(IMAGE_DIRECTORY,  \"gt\", \"*ground*\")))\n",
    "non_occ_files = sorted(glob.glob(os.path.join(IMAGE_DIRECTORY,  \"gt\", \"*nonocc*\")))\n",
    "unknown_files = sorted(glob.glob(os.path.join(IMAGE_DIRECTORY,  \"gt\", \"*unknown*\")))\n",
    "\n",
    "##################################################################\n",
    "# Loading benchmarked results into DataFrame#######################\n",
    "###################################################################\n",
    "\n",
    "benchmark_results_path = os.path.join(\"..\", \"..\", \"tests\", \"metrix_test\", \n",
    "                                      \"metrics_test_check_result.csv\")\n",
    "if not os.path.exists(benchmark_results_path):\n",
    "    raise Exception(\"Benchmarked results data file does not exist.\")\n",
    "    \n",
    "###################################################################\n",
    "# Note that the local dataframe's keys are initialised to 4x of ###\n",
    "# the original metrics.############################################\n",
    "###################################################################\n",
    "\n",
    "local_dataframe_data = dict(ALG_NAME=[], IMG=[], NONOCC=[])\n",
    "local_dataframe_data[\"BAD_2.0\"] = []\n",
    "local_dataframe_data[\"BAD_4.0\"] = []\n",
    "local_dataframe_data[\"BAD_8.0\"] = []\n",
    "    \n",
    "\n",
    "online_dataframe = pd.read_csv(benchmark_results_path).dropna()\n",
    "online_dataframe[\"ALG_NAME\"] = online_dataframe[\"ALG_NAME\"].str.strip()\n",
    "online_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw (GCC_PLUS_LOCALEXP_cones-05): bad1:0.0605 bad2:0.0346, bad4:0.0272, bad8:0.0233\n",
      "raw (SEGAGGR_cones-075): bad1:0.1440 bad2:0.0306, bad4:0.0216, bad8:0.0175\n",
      "raw (IGSM_cones-10): bad1:0.2573 bad2:0.0617, bad4:0.0214, bad8:0.0158\n",
      "raw (MULTIRBF_cones-15): bad1:0.4189 bad2:0.1764, bad4:0.0290, bad8:0.0059\n",
      "raw (MULTIRBF_cones-20): bad1:0.4189 bad2:0.1764, bad4:0.0290, bad8:0.0059\n",
      "raw (GCC_PLUS_LOCALEXP_teddy-05): bad1:0.0931 bad2:0.0516, bad4:0.0333, bad8:0.0245\n",
      "raw (SEGAGGR_teddy-075): bad1:0.1426 bad2:0.0433, bad4:0.0219, bad8:0.0123\n",
      "raw (IGSM_teddy-10): bad1:0.3151 bad2:0.0902, bad4:0.0408, bad8:0.0247\n",
      "raw (MULTIRBF_teddy-15): bad1:0.4135 bad2:0.1895, bad4:0.0509, bad8:0.0189\n",
      "raw (MULTIRBF_teddy-20): bad1:0.4135 bad2:0.1895, bad4:0.0509, bad8:0.0189\n",
      "raw (MULTIRBF_tsukaba-20): bad1:0.0305 bad2:0.0300, bad4:0.0291, bad8:0.0276\n",
      "raw (GCC_PLUS_LOCALEXP_venus-05): bad1:0.1091 bad2:0.0276, bad4:0.0035, bad8:0.0013\n",
      "raw (SEGAGGR_venus-075): bad1:0.1628 bad2:0.0323, bad4:0.0028, bad8:0.0012\n",
      "raw (IGSM_venus-10): bad1:0.6173 bad2:0.3742, bad4:0.0552, bad8:0.0007\n",
      "raw (MULTIRBF_venus-15): bad1:0.6211 bad2:0.3951, bad4:0.0838, bad8:0.0013\n",
      "raw (MULTIRBF_venus-20): bad1:0.6211 bad2:0.3951, bad4:0.0838, bad8:0.0013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################################################\n",
    "# Evaluation of the saved disparity maps ##########################\n",
    "###################################################################\n",
    "\n",
    "\n",
    "for gt, non_occ, unknown in zip(gt_files, non_occ_files, unknown_files):\n",
    "    \n",
    "    key = gt.split(os.path.sep)[-1].split(\".\")[0].split(\"_\")[0]\n",
    "  \n",
    "\n",
    "    \n",
    "    occ_key = non_occ.split(os.path.sep)[-1].split(\".\")[0].split(\"_\")[0]\n",
    "    \n",
    "    assert (key == occ_key), \"The key and occlusion key does not match. Please use a dictionary instead. \\n%s: %s\"%(key, occ_key)\n",
    "    \n",
    "    gt_raw = cv2.imread(gt, cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "    \n",
    "    nonocc_loaded = cv2.imread(non_occ, cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "\n",
    "    unknown_loaded = cv2.imread(unknown, cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "\n",
    "    for disp_map in image_dict[key]:\n",
    "        \n",
    "        # Submitted result\n",
    "        \n",
    "        disp_raw = cv2.imread(disp_map, cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "        \n",
    "        label = disp_map.split(os.path.sep)[-1].split(\".\")[0]\n",
    "        \n",
    "        threshold = float(re.search(\"[0-9]+\\.[0-9]+\", disp_map)[0])\n",
    "\n",
    "\n",
    "            # this is probably wrong...\n",
    "        filename = os.path.split(disp_map)[-1]\n",
    "        alg_name, scene, threshold = filename[0:filename.rfind(\"_\")], disp_map[int(disp_map.rfind(\"_\")+1):disp_map.rfind(\"-\")], disp_map[disp_map.rfind(\"-\"):]\n",
    "\n",
    "        # do both occluded and nonoccluded thresholds...\n",
    "        \n",
    "        MULTIPLIER = 1\n",
    "        OFFSET = 0\n",
    "        BAD1, BAD2, BAD4, BAD8, ABS_ERR, MSE, AVG, EUCLEDIAN = me.evaluate_over_all(disp_raw*MULTIPLIER, gt_raw, nonocc_loaded, occlusions_counted_in_errors = False)\n",
    "        formatted_str = \"raw ({4}): bad1:{0:.4f} bad2:{1:.4f}, bad4:{2:.4f}, bad8:{3:.4f}\".format(BAD1, BAD2, BAD4, \n",
    "                                                                                               BAD8,  label)\n",
    "        local_dataframe_data[\"ALG_NAME\"].append(alg_name)\n",
    "        local_dataframe_data[\"IMG\"].append(scene)\n",
    "        local_dataframe_data[\"NONOCC\"].append(1)\n",
    "        local_dataframe_data[\"BAD_2.0\"].append(BAD2)\n",
    "        local_dataframe_data[\"BAD_4.0\"].append(BAD4)\n",
    "        local_dataframe_data[\"BAD_8.0\"].append(BAD8)\n",
    "\n",
    "        print(formatted_str)\n",
    "        main_label =  \"raw ({4}):\\n bad1:{0:.4f} bad2:{1:.4f}, bad4:{2:.4f}, bad8:{3:.4f}\".format(BAD1, BAD2, BAD4, \n",
    "                                                                                                  BAD8,  label)\n",
    "        \n",
    "        BAD1, BAD2, BAD4, BAD8, ABS_ERR, MSE, AVG, EUCLEDIAN = me.evaluate_over_all(disp_raw*MULTIPLIER, gt_raw, unknown_loaded, occlusions_counted_in_errors = False)\n",
    "        formatted_str = \"raw ({4}): bad1:{0:.4f} bad2:{1:.4f}, bad4:{2:.4f}, bad8:{3:.4f}\".format(BAD1, BAD2, BAD4, \n",
    "                                                                                                  BAD8,  label)\n",
    "        local_dataframe_data[\"ALG_NAME\"].append(alg_name)\n",
    "        local_dataframe_data[\"IMG\"].append(scene)\n",
    "        local_dataframe_data[\"NONOCC\"].append(0)\n",
    "        local_dataframe_data[\"BAD_2.0\"].append(BAD2)\n",
    "        local_dataframe_data[\"BAD_4.0\"].append(BAD4)\n",
    "        local_dataframe_data[\"BAD_8.0\"].append(BAD8)\n",
    "\n",
    "        #plu.plot_images(list([disp_raw, gt_raw, nonocc_loaded]), list([main_label, label+\"_gt\", label+\"_occl\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32 entries, 0 to 31\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   ALG_NAME  32 non-null     object \n",
      " 1   IMG       32 non-null     object \n",
      " 2   NONOCC    32 non-null     int64  \n",
      " 3   BAD_2.0   32 non-null     float64\n",
      " 4   BAD_4.0   32 non-null     float64\n",
      " 5   BAD_8.0   32 non-null     float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 1.6+ KB\n",
      "----------------------------------------------------------\n",
      "Alg: GCC_PLUS_LOCALEXP, scene: CONES, nonocc: True, \n",
      "bad 0.5 ([3.5]) vs local bad 2.0 (3.5) match: True\n",
      "bad 1.0 ([2.7]) vs local bad 4.0 (2.7) match: True\n",
      "bad 2.0 ([2.3]) vs local bad 8.0 (2.3) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: GCC_PLUS_LOCALEXP, scene: CONES, nonocc: False, \n",
      "bad 0.5 ([8.7]) vs local bad 2.0 (8.7) match: True\n",
      "bad 1.0 ([7.4]) vs local bad 4.0 (7.4) match: True\n",
      "bad 2.0 ([6.3]) vs local bad 8.0 (6.3) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: SEGAGGR, scene: CONES, nonocc: True, \n",
      "bad 0.5 ([3.1]) vs local bad 2.0 (3.1) match: True\n",
      "bad 1.0 ([2.2]) vs local bad 4.0 (2.2) match: True\n",
      "bad 2.0 ([1.8]) vs local bad 8.0 (1.7) match: False\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: SEGAGGR, scene: CONES, nonocc: False, \n",
      "bad 0.5 ([8.6]) vs local bad 2.0 (8.6) match: True\n",
      "bad 1.0 ([6.5]) vs local bad 4.0 (6.5) match: True\n",
      "bad 2.0 ([5.2]) vs local bad 8.0 (5.2) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: IGSM, scene: CONES, nonocc: True, \n",
      "bad 0.5 ([6.2]) vs local bad 2.0 (6.2) match: True\n",
      "bad 1.0 ([2.1]) vs local bad 4.0 (2.1) match: True\n",
      "bad 2.0 ([1.6]) vs local bad 8.0 (1.6) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: IGSM, scene: CONES, nonocc: False, \n",
      "bad 0.5 ([11.8]) vs local bad 2.0 (11.8) match: True\n",
      "bad 1.0 ([7.]) vs local bad 4.0 (7.0) match: True\n",
      "bad 2.0 ([5.7]) vs local bad 8.0 (5.7) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: CONES, nonocc: True, \n",
      "bad 0.5 ([17.6]) vs local bad 2.0 (17.6) match: True\n",
      "bad 1.0 ([2.9]) vs local bad 4.0 (2.9) match: True\n",
      "bad 2.0 ([0.6]) vs local bad 8.0 (0.6) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: CONES, nonocc: False, \n",
      "bad 0.5 ([21.9]) vs local bad 2.0 (21.9) match: True\n",
      "bad 1.0 ([6.8]) vs local bad 4.0 (6.8) match: True\n",
      "bad 2.0 ([3.6]) vs local bad 8.0 (3.6) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: CONES, nonocc: True, \n",
      "bad 0.5 ([17.6]) vs local bad 2.0 (17.6) match: True\n",
      "bad 1.0 ([2.9]) vs local bad 4.0 (2.9) match: True\n",
      "bad 2.0 ([0.6]) vs local bad 8.0 (0.6) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: CONES, nonocc: False, \n",
      "bad 0.5 ([21.9]) vs local bad 2.0 (21.9) match: True\n",
      "bad 1.0 ([6.8]) vs local bad 4.0 (6.8) match: True\n",
      "bad 2.0 ([3.6]) vs local bad 8.0 (3.6) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: GCC_PLUS_LOCALEXP, scene: TEDDY, nonocc: True, \n",
      "bad 0.5 ([5.2]) vs local bad 2.0 (5.2) match: True\n",
      "bad 1.0 ([3.3]) vs local bad 4.0 (3.3) match: True\n",
      "bad 2.0 ([2.5]) vs local bad 8.0 (2.4) match: False\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: GCC_PLUS_LOCALEXP, scene: TEDDY, nonocc: False, \n",
      "bad 0.5 ([7.7]) vs local bad 2.0 (7.7) match: True\n",
      "bad 1.0 ([4.9]) vs local bad 4.0 (4.9) match: True\n",
      "bad 2.0 ([3.3]) vs local bad 8.0 (3.3) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: SEGAGGR, scene: TEDDY, nonocc: True, \n",
      "bad 0.5 ([4.3]) vs local bad 2.0 (4.3) match: True\n",
      "bad 1.0 ([2.2]) vs local bad 4.0 (2.2) match: True\n",
      "bad 2.0 ([1.2]) vs local bad 8.0 (1.2) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: SEGAGGR, scene: TEDDY, nonocc: False, \n",
      "bad 0.5 ([7.8]) vs local bad 2.0 (7.8) match: True\n",
      "bad 1.0 ([3.7]) vs local bad 4.0 (3.7) match: True\n",
      "bad 2.0 ([1.9]) vs local bad 8.0 (1.9) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: IGSM, scene: TEDDY, nonocc: True, \n",
      "bad 0.5 ([9.]) vs local bad 2.0 (9.0) match: True\n",
      "bad 1.0 ([4.1]) vs local bad 4.0 (4.1) match: True\n",
      "bad 2.0 ([2.5]) vs local bad 8.0 (2.5) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: IGSM, scene: TEDDY, nonocc: False, \n",
      "bad 0.5 ([12.1]) vs local bad 2.0 (12.1) match: True\n",
      "bad 1.0 ([6.]) vs local bad 4.0 (6.0) match: True\n",
      "bad 2.0 ([3.9]) vs local bad 8.0 (3.9) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: TEDDY, nonocc: True, \n",
      "bad 0.5 ([18.9]) vs local bad 2.0 (18.9) match: True\n",
      "bad 1.0 ([5.1]) vs local bad 4.0 (5.1) match: True\n",
      "bad 2.0 ([1.9]) vs local bad 8.0 (1.9) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: TEDDY, nonocc: False, \n",
      "bad 0.5 ([21.1]) vs local bad 2.0 (21.1) match: True\n",
      "bad 1.0 ([6.4]) vs local bad 4.0 (6.4) match: True\n",
      "bad 2.0 ([2.6]) vs local bad 8.0 (2.6) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: TEDDY, nonocc: True, \n",
      "bad 0.5 ([18.9]) vs local bad 2.0 (18.9) match: True\n",
      "bad 1.0 ([5.1]) vs local bad 4.0 (5.1) match: True\n",
      "bad 2.0 ([1.9]) vs local bad 8.0 (1.9) match: True\n",
      "\n",
      "----------------------------------------------------------\n",
      "Alg: MULTIRBF, scene: TEDDY, nonocc: False, \n",
      "bad 0.5 ([21.1]) vs local bad 2.0 (21.1) match: True\n",
      "bad 1.0 ([6.4]) vs local bad 4.0 (6.4) match: True\n",
      "bad 2.0 ([2.6]) vs local bad 8.0 (2.6) match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################################################\n",
    "# Equivalence test between local and published results ############\n",
    "###################################################################\n",
    "\n",
    "\n",
    "local_dataframe= pd.DataFrame(local_dataframe_data)\n",
    "local_dataframe.info()\n",
    "\n",
    "filtered_local_dataframe = local_dataframe[(local_dataframe[\"IMG\"]==\"teddy\") | (local_dataframe[\"IMG\"]==\"cones\")]\n",
    "\n",
    "errors = []\n",
    "for row in filtered_local_dataframe.values:\n",
    "    #print(\"row values: {0}, {1}, {2}\".format(row[0], row[1].upper(), bool(row[2])))\n",
    "    #print(row)\n",
    "    temp_online = online_dataframe[(online_dataframe[\"ALG_NAME\"] == row[0]) &\n",
    "                        (online_dataframe[\"IMG\"] == row[1].upper()) &\n",
    "                        (online_dataframe[\"NONOCCLUDED\"] == bool(row[2]))]\n",
    "    if(temp_online.size==0):\n",
    "        raise Exception(\"Missing entry in the downloaded data. ALG_NAME: {0}, IMG: {1}, NONOCC: {2}\".format(row[0], row[1], row[2]))\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    obad05 = temp_online[[\"BAD_0.5\"]].values[0]\n",
    "    obad10 = temp_online[[\"BAD_1.0\"]].values[0]\n",
    "    obad20 = temp_online[[\"BAD_2.0\"]].values[0]\n",
    "    lbad2 = round(row[3]*100, 1)\n",
    "    lbad4 = round(row[4]*100, 1)\n",
    "    lbad8= round(row[5]*100, 1)\n",
    "\n",
    "\n",
    "\n",
    "    m1 = lbad2 == obad05[0]\n",
    "    m2 = lbad4 == obad10[0]\n",
    "    m3 = lbad8 == obad20[0]\n",
    "\n",
    "    if not (m1 and m2 and m3):\n",
    "        errors.append([temp_online[\"ALG_NAME\"].iloc[0],\n",
    "                  temp_online[\"IMG\"].iloc[0],\n",
    "                  bool(temp_online[\"NONOCCLUDED\"].iloc[0] ),\n",
    "                  m1,\n",
    "                  m2,\n",
    "                  m3,\n",
    "                  obad05,\n",
    "                  lbad2,\n",
    "                  obad10,\n",
    "                  lbad4,\n",
    "                  obad20,\n",
    "                  lbad8])\n",
    "    message = \"Alg: {0},\" \\\n",
    "             \" scene: {1},\" \\\n",
    "             \" nonocc: {2}, \\n\"\\\n",
    "          \"bad 0.5 ({6}) vs local bad 2.0 ({7}) match: {3}\\n\"\\\n",
    "          \"bad 1.0 ({8}) vs local bad 4.0 ({9}) match: {4}\\n\"\\\n",
    "          \"bad 2.0 ({10}) vs local bad 8.0 ({11}) match: {5}\\n\"\\\n",
    "          .format(\n",
    "                  temp_online[\"ALG_NAME\"].iloc[0],\n",
    "                  temp_online[\"IMG\"].iloc[0],\n",
    "                  bool(temp_online[\"NONOCCLUDED\"].iloc[0] ),\n",
    "                  m1,\n",
    "                  m2,\n",
    "                  m3,\n",
    "                  obad05,\n",
    "                  lbad2,\n",
    "                  obad10,\n",
    "                  lbad4,\n",
    "                  obad20,\n",
    "                  lbad8\n",
    "    )\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Discrepancy found ---------------------------------------\n",
      "Alg: SEGAGGR, scene: CONES, nonocc: True, \n",
      "bad 0.5 ([3.1]) vs local bad 2.0 (3.1) match: True\n",
      "bad 1.0 ([2.2]) vs local bad 4.0 (2.2) match: True\n",
      "bad 2.0 ([1.8]) vs local bad 8.0 (1.7) match: False\n",
      "\n",
      "------------------------------Discrepancy found ---------------------------------------\n",
      "Alg: GCC_PLUS_LOCALEXP, scene: TEDDY, nonocc: True, \n",
      "bad 0.5 ([5.2]) vs local bad 2.0 (5.2) match: True\n",
      "bad 1.0 ([3.3]) vs local bad 4.0 (3.3) match: True\n",
      "bad 2.0 ([2.5]) vs local bad 8.0 (2.4) match: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for err in errors:\n",
    "    message = \"------------------------------Discrepancy found ---------------------------------------\\n\" \\\n",
    "              \"Alg: {0},\" \\\n",
    "              \" scene: {1},\" \\\n",
    "              \" nonocc: {2}, \\n\" \\\n",
    "              \"bad 0.5 ({6}) vs local bad 2.0 ({7}) match: {3}\\n\" \\\n",
    "              \"bad 1.0 ({8}) vs local bad 4.0 ({9}) match: {4}\\n\" \\\n",
    "              \"bad 2.0 ({10}) vs local bad 8.0 ({11}) match: {5}\\n\" \\\n",
    "        .format(\n",
    "        err[0],\n",
    "        err[1],\n",
    "        err[2],\n",
    "        err[3],\n",
    "        err[4],\n",
    "        err[5],\n",
    "        err[6],\n",
    "        err[7],\n",
    "        err[8],\n",
    "        err[9],\n",
    "        err[10],\n",
    "        err[11]\n",
    "    )\n",
    "    print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
