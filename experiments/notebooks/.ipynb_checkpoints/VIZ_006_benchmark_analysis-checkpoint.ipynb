{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be revised. Balderdash warning\n",
    "<h1 id=\"Visualisation-tools-for-Middlebury-2014-results\">Visualisation tools for Middlebury 2014 results</h1>\n",
    "<p>Admittedly, this dataset proved to be more challenging. Not just because of the higher number of the scenes and their more challenging dimensions, but:</p>\n",
    "<p>The disparity maps provided were in \"pfm\" format. This image format allows for 24 bit floating point image representation. However, even using the provided calibration files to each scene, transforming them to integer disparities proved to be challenging. The provided calibration files for each dataset are supposed to give information on the disparity ranges, minimum and maximum values that are encoded in each image.</p>\n",
    "This was used to scale these disparities to the level the algorithm outputs.\n",
    "To test the accuracy of converting the floating point disparities to integer values the following was done:</p>\n",
    "<ol>\n",
    "    <li>\n",
    "A scene, namely \"Teddy\" was converted to integer (8bit) values based on the provided configuration file.</li>\n",
    "    <li>As this is a scene present in both the 2003 and 2014 datasets, it had integer disparity maps (version 2003) and floating point ones (version 2014).<pre>Please note that here, \"integer values\" means the values saved in the disparity map file, not actual disparity values, as the 2003 version encoded disparities with 8 bit at a quater pixel accuracy.</pre>\n",
    "</li>\n",
    "    <li>The two versions disparity maps were compared.</li>\n",
    "    <li>Equivalence, apart from a small percentage of pixels could not be established.</li>\n",
    "    <li>There was an average of 2px difference between the two versions of the same frame.</li>\n",
    "</ol>        \n",
    "Please regard this as a \"caveat\" as using the old framework and old images (\"Teddy\" scenes) the algorithm scored a 9% bad2 value, whereas here it is significantly, typicall by at least factor of 2 greater. The reason for this is the aforementioned conversion error to which the reason is unknown.</p>\n",
    "<h2 id=\"Contents\">Contents</h2>\n",
    "<ol>\n",
    "<li>Scenes w.r.t. {metric} (selection plot)</li>\n",
    "<li>Scenes w.r.t. {metric} with color coded \"epochs\"</li>\n",
    "<li>\"Epochs w.r.t. {metric} with color coded scenes</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(\"..\", \"..\"))\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from ipywidgets import HBox, VBox, Button\n",
    "\n",
    "from components.utils import plotly_helpers as ph\n",
    "from components.utils import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "available_metrix = ['abs_error',\n",
    "       'mse', 'avg', 'eucledian', 'bad1', 'bad2', 'bad4', 'bad8']\n",
    "\n",
    "metrics_selector = widgets.Dropdown(\n",
    "    options=[(m,m) for m in available_metrix],\n",
    "    description='Metrics:',\n",
    "    value=\"bad2\"\n",
    ")\n",
    "\n",
    "\n",
    "nonoccluded = widgets.Dropdown(\n",
    "    options=[(\"yes\", False), (\"No\", True)],\n",
    "    description='Nonoccluded:'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please select metrics and whether occlusions are counted as errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8467a7d73fbf45f9995c8b38ae1b66f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Metrics:', index=5, options=(('abs_error', 'abs_error'), ('mse', 'mse'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VBox([metrics_selector, nonoccluded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_file = os.path.join(\"..\",\"..\", \"benchmarking\", \"MiddEval\", \"custom_log\", \"bm_benchmarking.csv\")\n",
    "#selected_file = \"./fixed_csv2.csv\"\n",
    "df = ph.load_n_clean(selected_file)\n",
    "\n",
    "##Filtering to selected occlusion parameter\n",
    "\n",
    "df = df[df[\"are_occlusions_errors\"]==nonoccluded.value]\n",
    "\n",
    "number_of_samples = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dashboard 1\n",
    "\n",
    "\n",
    "from ipywidgets import Image, Layout\n",
    "\n",
    "img_widget = Image(value=df[\"loaded_imgs\"].iloc[0], \n",
    "                   layout=Layout(height='375px', width='450px'))\n",
    "\n",
    "fig_a = ph.get_figure_widget (df, \"scene\", metrics_selector.value, \n",
    "                           \"Scene w.r.t.\"+metrics_selector.value)\n",
    "fig_b = ph.get_figure_widget (df, \"match\", \"kernel_size\", \"Kernel sizes w.r.t. match values\")\n",
    "\n",
    "\n",
    "figs = [fig_a, fig_b]\n",
    "ph.bind_hover_function(figs, img_widget, df)\n",
    "ph.bind_brush_function(figs, df)\n",
    "\n",
    "button = ph.get_reset_brush_button(figs)\n",
    "dashboard1 = VBox([button, fig_a,\n",
    "                  HBox([img_widget, fig_b])])\n",
    "\n",
    "\n",
    "### Dashboard 2\n",
    "\n",
    "df.sort_values(by=[\"experiment_id\"])\n",
    "traced_fig_1, dfs_1 = ph.get_figure_widget_traced(df, \"scene\", metrics_selector.value, \"experiment_id\")\n",
    "\n",
    "traced_fig_widget_1 = go.FigureWidget(traced_fig_1)\n",
    "\n",
    "\n",
    "\n",
    "traced_fig_1_imw_1 = Image(value=df[\"loaded_imgs\"].iloc[0], \n",
    "                   layout=Layout(height='375px', width='450px'))\n",
    "traced_fig_1_imw_2 = Image(value=df[\"loaded_gts\"].iloc[0], \n",
    "                   layout=Layout(height='375px', width='450px'))\n",
    "\n",
    "#figs, img_widget, selected_scene_df\n",
    "ph.bind_hover_function2([traced_fig_widget_1], traced_fig_1_imw_1, dfs_1, img_widget_groundtruth=traced_fig_1_imw_2)\n",
    "\n",
    "\n",
    "turn_the_lights_on = ph.get_dropdown_widget([\"On\", \"Off\"], label=\"Turn plots:\", values = [True, False])\n",
    "\n",
    "ph.bind_dropdown_switch_traces_fn(turn_the_lights_on, traced_fig_widget_1)\n",
    "\n",
    "dashboard2 = VBox([turn_the_lights_on, traced_fig_widget_1, HBox([traced_fig_1_imw_1,traced_fig_1_imw_2])])\n",
    "\n",
    "\n",
    "### Dashboard 3\n",
    "\n",
    "\n",
    "traced_fig_2, dfs_2 = ph.get_figure_widget_traced(df, \"experiment_id\", metrics_selector.value, \"scene\")\n",
    "\n",
    "traced_fig_widget_2 = go.FigureWidget(traced_fig_2)\n",
    "\n",
    "traced_fig_2_imw_1 = Image(value=df[\"loaded_imgs\"].iloc[0], \n",
    "                   layout=Layout(height='375px', width='450px'))\n",
    "traced_fig_2_imw_2 = Image(value=df[\"loaded_gts\"].iloc[0], \n",
    "                   layout=Layout(height='375px', width='450px'))\n",
    "\n",
    "\n",
    "\n",
    "#figs, img_widget, selected_scene_df\n",
    "ph.bind_hover_function2([traced_fig_widget_2], traced_fig_2_imw_1, dfs_2, img_widget_groundtruth=traced_fig_2_imw_2)\n",
    "\n",
    "turn_the_lights_on_2 = ph.get_dropdown_widget([\"On\", \"Off\"], label=\"Turn plots:\", values = [True, False])\n",
    "\n",
    "ph.bind_dropdown_switch_traces_fn(turn_the_lights_on_2, traced_fig_widget_2)\n",
    "\n",
    "\n",
    "dashboard3 = VBox([turn_the_lights_on_2, traced_fig_widget_2, HBox([traced_fig_2_imw_1,traced_fig_2_imw_2])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene w.r.t. {metric} (selection plot)\n",
    "<ol>\n",
    "    <li>The following figure allows to use the \"lasso\" tool as a tool of selection.</li>\n",
    "    <li>As a result, the relevant datapoints and their corresponding values in the figure in the bottom right corner will be highlighted.</li>\n",
    "    <li>Pressing the \"clear selection\" button will reset the figure.</li>\n",
    "    <li> Additionally, if a data point is hovered, the corresponding disparity output value will be displayed in the bottom right corner.</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c25fc7525943ec822713d74db3e08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='clear selection', style=ButtonStyle()), FigureWidget({\n",
       "    'data': [{'custo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dashboard1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenes w.r.t. {metric} with color coded \"epochs\"\n",
    "An \"epoch\" in this context means an experiment with the same settings evaluated across every scene in the Middlebury 2004 training dataset.<br>\n",
    "<ol>\n",
    "    <li>The following figure allows to turn all the plots on and off</li>\n",
    "    <li>Additionally, their visibiliy can also be handled by interacting with their legend entries on the right side of the plot.\n",
    "    </li>\n",
    "    <li> Therefore custom comparison can be made between different scenes, kernel sizes and match values. </li>\n",
    "    <li> TThe figure in the bottom left corner shows the corresponding disparity map. </li>\n",
    "    <li> TThe figure in the bottom right corner shows the corresponding ground truth disparity map. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e046c7f9cfb414b961fce0373ad9231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Turn plots:', options=(('On', True), ('Off', False)), value=True), Figure…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dashboard2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Epoch\" w.r.t. {metric} with color coded scenes\n",
    "An \"epoch\" in this context means an experiment with the same settings evaluated across every scene in the Middlebury 2004 training dataset.<br>\n",
    "<ol>\n",
    "    <li>The following figure allows to turn all the plots on and off</li>\n",
    "    <li>Additionally, their visibiliy can also be handled by interacting with their legend entries on the right side of the plot.\n",
    "    </li>\n",
    "    <li> Therefore custom comparison can be made between different scenes, kernel sizes and match values. </li>\n",
    "    <li> TThe figure in the bottom left corner shows the corresponding disparity map. </li>\n",
    "    <li> TThe figure in the bottom right corner shows the corresponding ground truth disparity map. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9ad49d898646938f7f033fd285eae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Turn plots:', options=(('On', True), ('Off', False)), value=True), Figure…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dashboard3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project_helpers\n",
    "df[\"fqfp\"] = [os.path.join(project_helpers.get_project_dir(), path) for path in df[\"image_filename\"]]\n",
    "df[\"fqfp_gt\"] = [os.path.join(project_helpers.get_project_dir(), os.path.dirname(path), \"disp0GT.pfm\") for path in df[\"image_filename\"]]\n",
    "df[\"fqfp_nonocc\"] = [os.path.join(project_helpers.get_project_dir(), os.path.dirname(path), \"mask0nocc.png\") for path in df[\"image_filename\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load disparity in pfm format\n",
    "\n",
    "df[\"loaded_gt_pfm\"] = np.array([u.load_pfm(path)[0] for path in df[\"fqfp_gt\"]])\n",
    "df[\"loaded_gt_pfm\"] = np.array([np.where(loaded_img==np.inf, 0, loaded_img)*4 for loaded_img in df[\"loaded_gt_pfm\"]])\n",
    "df[\"loaded_nonocc\"] =  np.array([cv2.imread(path, cv2.IMREAD_GRAYSCALE) for path in df[\"fqfp_nonocc\"]])\n",
    "df[\"loaded_img_cv2\"] = np.array([cv2.imread(path, cv2.IMREAD_GRAYSCALE) for path in df[\"fqfp\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['experiment_id', 'match', 'gap', 'egap', 'algo', 'init_method',\n",
       "       'dataset', 'scene', 'image_filename', 'img_res', 'preprocessing_method',\n",
       "       'kernel_size', 'kernel_spec', 'are_occlusions_errors', 'abs_error',\n",
       "       'mse', 'avg', 'eucledian', 'bad1', 'bad2', 'bad4', 'bad8', 'runtime',\n",
       "       'loaded_imgs', 'loaded_gts', 'h', 'w', 'fqfp', 'fqfp_gt', 'fqfp_nonocc',\n",
       "       'loaded_gt_pfm', 'loaded_nonocc', 'loaded_img_cv2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.utils.Metrix import Wrapper as me\n",
    "from skimage.filters import median\n",
    "def eval_fix_helper(args):\n",
    "    disp, gt, occ = args[0], args[1], args[2]\n",
    "    disp=median(disp)\n",
    "    return me.evaluate_over_all(disp, gt, occ, occlusions_counted_in_errors = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, data in df[[\"loaded_img_cv2\", \"loaded_gt_pfm\", \"loaded_nonocc\"]].iterrows():\n",
    "    df.loc[index, [\"bad1\",\"bad2\",\"bad4\",\"bad8\",\"abs_error\",\"mse\",\"avg\",\"eucledian\"]] = eval_fix_helper(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(df, values = \"bad4\", index=\"experiment_id\", aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>plusblg_40_5x7gc_3_gs_1_alph_0</th>\n",
       "      <td>0.314936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plusblg_40_7x3gc_3_gs_1_alph_0</th>\n",
       "      <td>0.317227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_40_5x7gc_8_gs_90_alph_0</th>\n",
       "      <td>0.318546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_40_7x3gc_8_gs_90_alph_0</th>\n",
       "      <td>0.320208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_30_7x3gc_8_gs_90_alph_0</th>\n",
       "      <td>0.321276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_30_3x5gc_8_gs_90_alph_0</th>\n",
       "      <td>0.321699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_30_5x7gc_8_gs_90_alph_0</th>\n",
       "      <td>0.321834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_50_5x7gc_8_gs_90_alph_0</th>\n",
       "      <td>0.322747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plusblg_40_7x3gc_3_gs_2_alph_0</th>\n",
       "      <td>0.325805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_50_7x3gc_8_gs_90_alph_0</th>\n",
       "      <td>0.325898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_40_3x5gc_8_gs_90_alph_0</th>\n",
       "      <td>0.327712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plusblg_40_3x5gc_3_gs_1_alph_0</th>\n",
       "      <td>0.327740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_40_5x7gc_10_gs_90_alph_0</th>\n",
       "      <td>0.327979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_30_5x7gc_10_gs_90_alph_0</th>\n",
       "      <td>0.328208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_30_7x3gc_10_gs_90_alph_0</th>\n",
       "      <td>0.328514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_40_7x3gc_10_gs_90_alph_0</th>\n",
       "      <td>0.329257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_50_3x5gc_8_gs_90_alph_0</th>\n",
       "      <td>0.330295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_50_5x7gc_10_gs_90_alph_0</th>\n",
       "      <td>0.332099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plusblg_40_5x7gc_2_gs_1_alph_0</th>\n",
       "      <td>0.334501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blg_50_7x3gc_10_gs_90_alph_0</th>\n",
       "      <td>0.335124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    bad4\n",
       "experiment_id                           \n",
       "plusblg_40_5x7gc_3_gs_1_alph_0  0.314936\n",
       "plusblg_40_7x3gc_3_gs_1_alph_0  0.317227\n",
       "blg_40_5x7gc_8_gs_90_alph_0     0.318546\n",
       "blg_40_7x3gc_8_gs_90_alph_0     0.320208\n",
       "blg_30_7x3gc_8_gs_90_alph_0     0.321276\n",
       "blg_30_3x5gc_8_gs_90_alph_0     0.321699\n",
       "blg_30_5x7gc_8_gs_90_alph_0     0.321834\n",
       "blg_50_5x7gc_8_gs_90_alph_0     0.322747\n",
       "plusblg_40_7x3gc_3_gs_2_alph_0  0.325805\n",
       "blg_50_7x3gc_8_gs_90_alph_0     0.325898\n",
       "blg_40_3x5gc_8_gs_90_alph_0     0.327712\n",
       "plusblg_40_3x5gc_3_gs_1_alph_0  0.327740\n",
       "blg_40_5x7gc_10_gs_90_alph_0    0.327979\n",
       "blg_30_5x7gc_10_gs_90_alph_0    0.328208\n",
       "blg_30_7x3gc_10_gs_90_alph_0    0.328514\n",
       "blg_40_7x3gc_10_gs_90_alph_0    0.329257\n",
       "blg_50_3x5gc_8_gs_90_alph_0     0.330295\n",
       "blg_50_5x7gc_10_gs_90_alph_0    0.332099\n",
       "plusblg_40_5x7gc_2_gs_1_alph_0  0.334501\n",
       "blg_50_7x3gc_10_gs_90_alph_0    0.335124"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot.sort_values(by=\"bad4\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./fixed_csv_median_filtered.csv\", columns=\"experiment_id,match,gap,egap,algo,\\\n",
    "init_method,dataset,scene,image_filename,img_res,preprocessing_method,\\\n",
    "kernel_size,kernel_spec,are_occlusions_errors,abs_error,mse,avg,eucledian,bad1,bad2,bad4,bad8,runtime\".split(\",\")\n",
    ", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
